# Estimating Today's Unemployment Rate

## Motivations
This codebase is motivated by recent advances in economic forecasting models and the
transition to the wide adoption of machine learning techniques. For decades, the general
approach to economic forecasting relied on improvements to autoregressive (AR) models
and time series econometrics. Disproportionate progress in the area of machine learning
and natural language processing has left significant opportunity to better existing models
with the help of such techniques.

In a similar vein and motivated largely in part by the coronavirus pandemic, there
has been an increased taste for high frequency variables to include in rolling forecasts.
This shift in demand was realized in many case count prediction models, which included
[mobility-based](https://covid-mobility.stanford.edu/) and [credit card spending](https://hbr.org/2020/11/a-better-model-for-economic-forecasting-during-the-pandemic) features, for example.

Given this context, there is particular opportunity in modernizing our approach to reporting
the [U3 unemployment rate](https://fred.stlouisfed.org/series/UNRATE), without sacrificing statistical certainty.
The measure is reported on a monthly lag, and like many labor market measures, is only an approximation of the current
level of unemployment. Hence, this project demonstrates the predictive power of [Google Trends](https://trends.google.com/trends/?geo=US) based indices which are generated by extracting sentiments from regarded public texts. Due to
sheer volume of data retrieved from such a database, hyperparameterization is implemented
in the form of Random Forest Regression and the Least Absolute Shrinkage and Selection Operator (LASSO).
These models are horse raced with both traditional AR models and recent modifications to such
models using similar techniques. The ultimate goal is to estimate the unemployment rate at a higher
temporal and geographic frequency than currently , and to outperform competing models.

## Methodology
The approach to this real-time prediction question is best characterized by a four part, ordered
methodological framework:
- Latent Dirichlet Allocation (LDA) on identified "key documents"
- High volume data retrieval using Google Trends API
- Data normalization [(West 2021)](https://arxiv.org/pdf/2007.13861.pdf), cleaning, and merging
- Model horse racing, performance measurements (RMSE)


## Installation
To use the code, clone the repository to a local directory. From the command line and in that
directory, type: `pip install -r requirements.txt`. This will install repository-specific dependencies.

From there, typing `python main.py` will run the main LDA topic-modeling, API request,
and regression functions. Further analysis is done in `analysis.py`.


## Data
* Unemployment and labor market measures are from the [US Bureau of Labor Statistics](https://www.bls.gov/).
* Google search query data are from [Google Trends](https://trends.google.com/trends/?geo=US).


## Notes
This project is the codebase for my senior thesis in the Department of Policy Analysis and Management
at Cornell University.
