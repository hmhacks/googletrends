# Estimating Today's Unemployment Rate

## Motivations
This codebase is motivated by recent advances in economic forecasting models and the
transition to the wide adoption of machine learning techniques. For decades, the general
approach to economic forecasting relied on improvements to autoregressive (AR) models
and time series econometrics. Disproportionate progress in the area of machine learning
and natural language processing has left significant opportunity to better existing models
with the help of such techniques.

In a similar vein and motivated largely in part by the coronavirus pandemic, there
has been an increased taste for high frequency variables to include in rolling forecasts.
This shift in demand was realized in many case count prediction models, which included
[mobility-based](https://covid-mobility.stanford.edu/) and [credit card spending](https://hbr.org/2020/11/a-better-model-for-economic-forecasting-during-the-pandemic) features, for example.

Given this context, there is particular opportunity in modernizing our approach to reporting
the [U3 unemployment rate](https://fred.stlouisfed.org/series/UNRATE), without sacrificing statistical certainty.
The measure is reported on a monthly lag, and like many labor market measures, is only an approximation of the current level of unemployment. Hence, this project demonstrates the predictive power of [Google Trends](https://trends.google.com/trends/?geo=US) based indices which are generated by extracting sentiments from regarded public texts. Existing work in this space ([D'Amuri& Marcucci 2017](https://ideas.repec.org/p/bdi/wptemi/td_891_12.html), [Varian & Choi 2012](https://onlinelibrary.wiley.com/doi/abs/10.1111/j.1475-4932.2012.00809.x)) is yet to wrangle with questions of high-dimensionality, yet alone include non-literally interpreted indices (e.g. "jobs",
"job openings") which excludes the possibility of proxies (e.g "flights", "travel") outperforming them. Similar exercises have been implemented to explore the power of such data in predicting the Italian youth unemployment rate [(Naccarato et. al 2018)](https://www.sciencedirect.com/science/article/abs/pii/S0040162517316049), national oil consumption [(Yu et. al 2019)](https://ideas.repec.org/a/eee/intfor/v35y2019i1p213-223.html)), and Swedish retail sales [(Lindberg 2011)](https://helda.helsinki.fi/handle/10138/153285), among many other endogenous variables.  

Due to the sheer volume of data retrieved from such a database, the current project implements hyperparameter optimization in the form of Random Forest Regression and the Least Absolute Shrinkage and Selection Operator (LASSO). These models are horse raced with both traditional AR models and recent modifications to such
models using similar techniques. The ultimate goal is to estimate the unemployment rate at a higher
temporal and geographic frequency than currently , and to outperform competing models.

## Methodology
The approach to this real-time prediction question is best characterized by a four part, ordered
methodological framework:
- Latent Dirichlet Allocation (LDA) on identified "key documents"
- High volume data retrieval using Google Trends API
- Data normalization [(West 2021)](https://arxiv.org/pdf/2007.13861.pdf), unit root testing, cleaning, and merging
- Model horse racing, performance measurements (RMSE)


## Installation
To use the code, clone the repository to a local directory. From the command line and in that
directory, type: `pip install -r requirements.txt`. This will install repository-specific dependencies.

From there, typing `python main.py` will run the main LDA topic-modeling, API request,
and regression functions. Further analysis is done in `analysis.py`.


## Data
* Unemployment and labor market measures are from the [US Bureau of Labor Statistics](https://www.bls.gov/).
* Google search query data are from [Google Trends](https://trends.google.com/trends/?geo=US).


## Notes
This project is the codebase for my senior thesis in the Department of Policy Analysis and Management
at Cornell University.
